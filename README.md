# Experiment 2 â€“ Binary Classification using NaÃ¯ve Bayes, KNN, and SVM

This repository contains **Experiment 2** from the *Machine Learning Algorithms Laboratory*.  
The experiment focuses on implementing and evaluating **NaÃ¯ve Bayes**, **K-Nearest Neighbors (KNN)**, and **Support Vector Machine (SVM)** classifiers for a **binary email spam classification problem**.

---

## ğŸ“Œ Experiment Details

- **Institution:** Sri Sivasubramaniya Nadar College of Engineering, Chennai  
- **Affiliation:** Anna University  
- **Degree & Branch:** B.E. Computer Science & Engineering  
- **Semester:** VI  
- **Subject Code & Name:** UCS2612 â€“ Machine Learning Algorithms Laboratory  
- **Academic Year:** 2025â€“2026 (Even Semester)  
- **Batch:** 2023â€“2027  

---

## ğŸ¯ Aim

To classify emails as **Spam** or **Ham** using:
- NaÃ¯ve Bayes
- K-Nearest Neighbors (KNN)
- Support Vector Machine (SVM)

and evaluate their performance using:
- Accuracy
- Precision
- Recall
- F1-score
- ROCâ€“AUC
- K-Fold Cross-Validation

---

## ğŸ§° Libraries Used

- **Pandas** â€“ Data manipulation  
- **NumPy** â€“ Numerical operations  
- **Scikit-learn** â€“ Model building, preprocessing, evaluation  
- **Matplotlib** â€“ Visualization  
- **Seaborn** â€“ Statistical visualization  

---

## ğŸ“‚ Dataset Used

- **Spambase Dataset**
- Binary classification:
  - `0` â†’ Ham (Not Spam)
  - `1` â†’ Spam

---

## ğŸ¤– Machine Learning Models Used

- **NaÃ¯ve Bayes**
  - GaussianNB
  - MultinomialNB
  - BernoulliNB
- **K-Nearest Neighbors (KNN)**
  - k = 1, 3, 5, 7
  - KDTree and BallTree
- **Support Vector Machine (SVM)**
  - Linear kernel
  - Polynomial kernel
  - RBF kernel
  - Sigmoid kernel

---

## ğŸ§ª Experiment Workflow

### 1ï¸âƒ£ Data Loading
- Load dataset using Pandas
- Check for missing values
- Separate features and labels

### 2ï¸âƒ£ Data Preprocessing
- Feature normalization using `StandardScaler`
- Trainâ€“test split with stratification

### 3ï¸âƒ£ Exploratory Data Analysis
- Class distribution bar chart
- Feature distribution histograms

### 4ï¸âƒ£ Model Training
- Train NaÃ¯ve Bayes variants
- Train KNN with different `k` values
- Compare KDTree vs BallTree
- Train SVM with multiple kernels

### 5ï¸âƒ£ Model Evaluation
- Classification report
- Confusion matrix
- ROC curve and AUC score
- Training time comparison

### 6ï¸âƒ£ K-Fold Cross-Validation
- 5-Fold cross-validation
- Compare average accuracy across models

---

## ğŸ“Š Performance Metrics Used

- Accuracy  
- Precision  
- Recall  
- F1-score  
- ROCâ€“AUC  
- Training Time  

---

## ğŸ“ˆ Output Visualizations

- Class distribution bar chart  
- Feature distribution histograms  
- Confusion matrices for all models  
- ROC curves with AUC values  
- KNN tree comparison plots  
- SVM kernel-wise performance table  
- 5-Fold cross-validation results  

---

## ğŸ” Observations

### âœ… Best Classifier
- **SVM (Linear Kernel)** achieved the **highest average accuracy: 0.9274**

### âœ… Best NaÃ¯ve Bayes Variant
- **Bernoulli NaÃ¯ve Bayes**
  - Accuracy: 0.8863
  - Highest AUC among NB variants

### âœ… KNN Performance
- Accuracy improved as `k` increased
- Best results at `k = 7`
- KDTree and BallTree gave similar accuracy
- BallTree trained slightly faster

### âœ… Best SVM Kernel
- **Linear kernel** performed best overall
- RBF kernel was a close second
- Polynomial kernel performed poorly

### âœ… Hyperparameter Influence
- KNN accuracy highly dependent on `k`
- SVM performance strongly dependent on kernel choice

---

## ğŸ§  Learning Outcomes

From this experiment, we learned:

- Practical implementation of NaÃ¯ve Bayes, KNN, and SVM
- Importance of feature scaling and preprocessing
- Effect of hyperparameters on model performance
- Use of evaluation metrics beyond accuracy
- Visualization using confusion matrices and ROC curves
- Importance of K-fold cross-validation
- Comparative analysis of multiple classifiers

---
